{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQDgrPj7ljbF"
      },
      "source": [
        "# ARR Tool: Track your ACs and Reviews\n",
        "\n",
        "This is a simple notebook that allows SAC to track the paper review status using OpenReview API.\n",
        "\n",
        "- check all papers & reviews under your batch\n",
        "- retrieve meta-review & status\n",
        "- retrieve confidential comments to editors, and review issue reports\n",
        "\n",
        "***Copy this notebook to run in your own environment.***\n",
        "\n",
        "Author: [Yiming Cui](https://ymcui.com/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "R2yOkw8K03Xu",
        "outputId": "7e3a79b5-43b1-4e7c-a25f-9137c6ad80dc"
      },
      "outputs": [],
      "source": [
        "!pip install openreview-py\n",
        "!pip install plotly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ufUTK3I1QZS"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import openreview\n",
        "import pandas as pd\n",
        "from prettytable import PrettyTable\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML\n",
        "from datetime import datetime\n",
        "import markdown as md\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#############################\n",
        "#UPDATE THIS CELL WITH YOUR OpenReview INFO\n",
        "username = \"your-email\" #@param {type:\"string\"}\n",
        "password = \"your-password\" #@param {type:\"string\"}\n",
        "me =\"~YourProfile\" #@param {type:\"string\"}\n",
        "#############################\n",
        "\n",
        "client = openreview.api.OpenReviewClient(baseurl='https://api2.openreview.net', username=username, password=password)\n",
        "\n",
        "venue_id = 'aclweb.org/ACL/ARR/2025/February' #@param {type:\"string\"}\n",
        "venue_group = client.get_group(venue_id)\n",
        "submission_name = venue_group.content['submission_name']['value']\n",
        "submissions = client.get_all_notes(invitation=f'{venue_id}/-/{submission_name}', details='replies')\n",
        "my_sac_groups = {\n",
        "    g.id\n",
        "    for g in client.get_all_groups(members=me, prefix=f'{venue_id}/{submission_name}')\n",
        "    if g.id.endswith('Senior_Area_Chairs')\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oblMpKWnIuyk"
      },
      "source": [
        "## Monitor paper review status (updated with meta-review, scores)\n",
        "\n",
        "This cell can track paper status, showing the number of completed reviews, assigned AC, meta-review status, etc.\n",
        "\n",
        "- paper #: the paper ID\n",
        "- paper ID: the paper ID (openreview.net/forum?id=...)\n",
        "- Paper Type: Long / Short\n",
        "- Area Chair: the area chair\n",
        "- Num Reviews: the number of completed reviews / expected reviews (may more than 3 if emergency reviewers are assigned)\n",
        "- Ready for Rebuttal: if received three or more reviews, it will be marked as \"‚àö\"\n",
        "- Author Response: if author response is available (we use a heuristic to count author comments >= 3)\n",
        "- Reviewer Scores: show average and individual reviewer scores (confidence/soundness/excitement/overall)\n",
        "- Meta Review Score: show meta-review score if it is ready\n",
        "\n",
        "**If you want to refresh the submission status (fetch up-to-date data from OR), you should run previous cell again.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iieM7J-Am6Tc"
      },
      "outputs": [],
      "source": [
        "# Helper function: Count a reply as an actual review only if its invitations include \"/-/Official_Review\"\n",
        "def is_actual_review(reply):\n",
        "    return any('/-/Official_Review' in invitation for invitation in reply.get('invitations', []))\n",
        "\n",
        "# Helper function: Check if a reply is a meta-review (assuming its invitation contains \"/-/Meta_Review\")\n",
        "def is_meta_review(reply):\n",
        "    return any('/-/Meta_Review' in invitation for invitation in reply.get('invitations', []))\n",
        "\n",
        "def is_withdrawn(submission):\n",
        "    # Check if there's a non-empty withdrawal_confirmation field.\n",
        "    withdrawal_conf = submission.content.get(\"withdrawal_confirmation\", {}).get(\"value\", \"\").strip()\n",
        "    if withdrawal_conf:\n",
        "        return True\n",
        "    # Alternatively, check if the venue value contains \"withdrawn\" (case insensitive).\n",
        "    venue_val = submission.content.get(\"venue\", {}).get(\"value\", \"\").lower()\n",
        "    if \"withdrawn\" in venue_val:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# Helper function to format a list of scores into \"avg_score (score1 / score2 / ...)\" with 1 decimal precision.\n",
        "def format_scores_as_list(scores):\n",
        "    if scores:\n",
        "        avg = sum(scores) / len(scores)\n",
        "        score_list = \" / \".join(f\"{s:.1f}\" for s in scores)\n",
        "        return f\"{avg:.1f} ({score_list})\"\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "# Collect data for each submission.\n",
        "data = []\n",
        "for submission in submissions:\n",
        "    # Skip withdrawn or desk rejected papers.\n",
        "    if is_withdrawn(submission):\n",
        "        print(f\"Skipping withdrawn paper: {submission.id}\")\n",
        "        continue\n",
        "    if \"venue\" in submission.content and \"desk rejected\" in submission.content[\"venue\"][\"value\"].lower():\n",
        "        print(f\"Skipping desk rejected paper: {submission.id}\")\n",
        "        continue\n",
        "\n",
        "    prefix = f'{venue_id}/{submission_name}{submission.number}'\n",
        "    # Process only submissions in your SAC batch.\n",
        "    if not (set(submission.readers) & my_sac_groups):\n",
        "        continue\n",
        "\n",
        "    # Retrieve the assigned Area Chair.\n",
        "    area_chairs_group = client.get_group(f'{prefix}/Area_Chairs')\n",
        "    if area_chairs_group.members:\n",
        "        ac = area_chairs_group.members[0]  # Assuming one AC per paper.\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "    # Extract Paper Type (e.g., \"Long\" or \"Short\").\n",
        "    paper_type = submission.content.get(\"paper_type\", {}).get(\"value\", \"\")\n",
        "\n",
        "    # Count the number of completed reviews (using our stricter filter).\n",
        "    completed_reviews = sum(1 for reply in submission.details[\"replies\"] if is_actual_review(reply))\n",
        "\n",
        "    # Determine the expected number of reviews from the Reviewers group.\n",
        "    expected_reviews = 0\n",
        "    try:\n",
        "        reviewers_group = client.get_group(f'{prefix}/Reviewers')\n",
        "        expected_reviews = len(reviewers_group.members)\n",
        "    except Exception:\n",
        "        expected_reviews = 0\n",
        "\n",
        "    # Merge the two into \"Num Reviews\" as \"x / y\".\n",
        "    num_reviews = f\"{completed_reviews} / {expected_reviews}\"\n",
        "\n",
        "    # Set review status: Checkmark if the paper has three or more completed reviews.\n",
        "    status = \"‚úì\" if completed_reviews >= 3 else \"\"\n",
        "\n",
        "    # Extract meta-review score from the meta-review reply, if available.\n",
        "    meta_review_score = \"\"\n",
        "    for reply in submission.details[\"replies\"]:\n",
        "        if is_meta_review(reply):\n",
        "            content = reply.get(\"content\", {})\n",
        "            if \"overall_assessment\" in content:\n",
        "                meta_review_score = content[\"overall_assessment\"].get(\"value\", \"\")\n",
        "            elif \"overall_rating\" in content:\n",
        "                meta_review_score = content[\"overall_rating\"].get(\"value\", \"\")\n",
        "            elif \"score\" in content:\n",
        "                meta_review_score = content[\"score\"].get(\"value\", \"\")\n",
        "            break\n",
        "\n",
        "    # Count author responses: count replies whose first signature indicates an author.\n",
        "    author_response_count = sum(1 for reply in submission.details.get(\"replies\", [])\n",
        "                                  if reply.get(\"signatures\", [])[0].find(\"/Authors\") != -1)\n",
        "    # If at least 3 author responses exist, mark with a checkmark.\n",
        "    author_response = \"‚úì\" if author_response_count >= 3 else \"\"\n",
        "\n",
        "    # Initialize lists to collect reviewer scores.\n",
        "    confidence_scores = []\n",
        "    soundness_scores = []\n",
        "    excitement_scores = []\n",
        "    overall_assessment_scores = []\n",
        "\n",
        "    # Aggregate reviewer scores from review replies.\n",
        "    for reply in submission.details[\"replies\"]:\n",
        "        if is_actual_review(reply):\n",
        "            content = reply.get(\"content\", {})\n",
        "            try:\n",
        "                val = content.get(\"confidence\", {}).get(\"value\", None)\n",
        "                if val is not None:\n",
        "                    confidence_scores.append(float(val))\n",
        "            except:\n",
        "                pass\n",
        "            try:\n",
        "                val = content.get(\"soundness\", {}).get(\"value\", None)\n",
        "                if val is not None:\n",
        "                    soundness_scores.append(float(val))\n",
        "            except:\n",
        "                pass\n",
        "            try:\n",
        "                val = content.get(\"excitement\", {}).get(\"value\", None)\n",
        "                if val is not None:\n",
        "                    excitement_scores.append(float(val))\n",
        "            except:\n",
        "                pass\n",
        "            try:\n",
        "                val = content.get(\"overall_assessment\", {}).get(\"value\", None)\n",
        "                if val is not None:\n",
        "                    overall_assessment_scores.append(float(val))\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    reviewer_confidence = format_scores_as_list(confidence_scores)\n",
        "    reviewer_soundness = format_scores_as_list(soundness_scores)\n",
        "    reviewer_excitement = format_scores_as_list(excitement_scores)\n",
        "    reviewer_overall = format_scores_as_list(overall_assessment_scores)\n",
        "\n",
        "    data.append({\n",
        "        \"Paper #\": submission.number,\n",
        "        \"Paper ID\": submission.id,\n",
        "        \"Paper Type\": paper_type,\n",
        "        \"Area Chair\": ac,\n",
        "        \"Num Reviews\": num_reviews,\n",
        "        \"Ready for Rebuttal\": status,\n",
        "        \"Author Response\": author_response,\n",
        "        \"Reviewer Confidence\": reviewer_confidence,\n",
        "        \"Soundness Score\": reviewer_soundness,\n",
        "        \"Excitement Score\": reviewer_excitement,\n",
        "        \"Overall Assessment\": reviewer_overall,\n",
        "        \"Meta Review Score\": meta_review_score\n",
        "    })\n",
        "\n",
        "# Create a DataFrame from the collected data.\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Reorder columns so that the new \"Num Reviews\" appears after \"Area Chair\".\n",
        "desired_order = [\n",
        "    \"Paper #\",\n",
        "    \"Paper ID\",\n",
        "    \"Paper Type\",\n",
        "    \"Area Chair\",\n",
        "    \"Num Reviews\",\n",
        "    \"Ready for Rebuttal\",\n",
        "    \"Author Response\",\n",
        "    \"Reviewer Confidence\",\n",
        "    \"Soundness Score\",\n",
        "    \"Excitement Score\",\n",
        "    \"Overall Assessment\",\n",
        "    \"Meta Review Score\"\n",
        "]\n",
        "df = df[desired_order]\n",
        "\n",
        "# --- Interactive Papers Table ---\n",
        "print(len(df))\n",
        "print(\"All Papers in Your Batch:\")\n",
        "display(df)\n",
        "\n",
        "# Create an interactive dropdown to filter by Area Chair.\n",
        "ac_options = [\"All\"] + sorted(df[\"Area Chair\"].unique())\n",
        "ac_dropdown = widgets.Dropdown(options=ac_options, description=\"Area Chair:\")\n",
        "output = widgets.Output()\n",
        "\n",
        "def update_table(change):\n",
        "    with output:\n",
        "        output.clear_output()\n",
        "        if ac_dropdown.value == \"All\":\n",
        "            display(df)\n",
        "        else:\n",
        "            filtered_df = df[df[\"Area Chair\"] == ac_dropdown.value]\n",
        "            display(filtered_df)\n",
        "\n",
        "ac_dropdown.observe(update_table, names=\"value\")\n",
        "\n",
        "print(\"Filter Papers by Area Chair:\")\n",
        "display(ac_dropdown)\n",
        "display(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1wJwcK4ON8k"
      },
      "source": [
        "### Visualization of Review / Meta-review Scores (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJbNCxluOUTm"
      },
      "outputs": [],
      "source": [
        "# Helper function to extract the average Overall Assessment from the formatted string.\n",
        "def parse_avg(s):\n",
        "    try:\n",
        "        return float(s.split()[0])\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "# Helper function to parse a meta review score into a float.\n",
        "def parse_meta_review(s):\n",
        "    try:\n",
        "        return float(s)\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "# Compute temporary Series (without adding to df)\n",
        "overall_assessment_avg = df[\"Overall Assessment\"].apply(parse_avg)\n",
        "meta_review_float = df[\"Meta Review Score\"].apply(parse_meta_review)\n",
        "\n",
        "# Define bins for Overall Assessment: from 1 to 5 (0.5 interval)\n",
        "bins = np.arange(1, 5.5, 0.5)  # bin edges: 1, 1.5, ..., 5, 5.5\n",
        "\n",
        "# Compute histogram counts for Overall Assessment\n",
        "overall_counts, overall_bin_edges = np.histogram(overall_assessment_avg.dropna(), bins=bins)\n",
        "overall_bin_centers = (overall_bin_edges[:-1] + overall_bin_edges[1:]) / 2\n",
        "\n",
        "# For Meta Review Scores, use value_counts (since each paper has exactly one meta review score)\n",
        "meta_counts_series = meta_review_float.dropna().value_counts().sort_index()\n",
        "x_meta_values = meta_counts_series.index.values   # exact score values, e.g., 1, 1.5, 2, ...\n",
        "y_meta_values = meta_counts_series.values\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot Overall Assessment distribution as a line chart.\n",
        "plt.plot(overall_bin_centers, overall_counts, marker='o', label=\"Overall Assessment\", color='steelblue')\n",
        "# Annotate each point for Overall Assessment.\n",
        "for x, y in zip(overall_bin_centers, overall_counts):\n",
        "    plt.annotate(f\"{y}\", xy=(x, y), xytext=(0, 5), textcoords=\"offset points\",\n",
        "                 ha=\"center\", fontsize=9, color='steelblue')\n",
        "\n",
        "# Plot Meta Review Score distribution as a line chart.\n",
        "plt.plot(x_meta_values, y_meta_values, marker='o', label=\"Meta Review Score\", color='red')\n",
        "# Annotate each point for Meta Review Score.\n",
        "for x, y in zip(x_meta_values, y_meta_values):\n",
        "    plt.annotate(f\"{y}\", xy=(x, y), xytext=(0, 5), textcoords=\"offset points\",\n",
        "                 ha=\"center\", fontsize=9, color='red')\n",
        "\n",
        "plt.xlabel(\"Score\")\n",
        "plt.ylabel(\"Number of Papers\")\n",
        "plt.title(\"Distribution of Overall Assessment and Meta Review Scores\")\n",
        "# Set x-axis ticks from 1 to 5 in increments of 0.5.\n",
        "plt.xticks(np.arange(1, 5.5, 0.5))\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUyGOvPHv-rL"
      },
      "source": [
        "### Distribution of Meta Reviews vs Overall Assessments (optional)\n",
        "\n",
        "*Thanks to [Desmond Elliott](http://elliottd.github.io/)'s code contribution!*\n",
        "\n",
        "I add some color and marker to individual AC to make the visualization more accessible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwJzPVVqv-SH"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import plotly.express as px\n",
        "\n",
        "# Assume parse_avg and parse_meta_review functions are defined.\n",
        "y_data = df[\"Overall Assessment\"].apply(parse_avg)\n",
        "# Jitter amount to help distinguish overlapping points.\n",
        "jitter_strength = 0.2 # @param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "dupes = Counter(y_data)\n",
        "y_data2 = [xi + np.random.uniform(-jitter_strength, jitter_strength) if dupes[xi] > 1 else xi for xi in y_data]\n",
        "\n",
        "df2 = pd.DataFrame({\n",
        "    'Meta Review Score': df[\"Meta Review Score\"].apply(parse_meta_review),\n",
        "    'Jittered Assessment': y_data2,\n",
        "    'Overall Assessment': y_data,\n",
        "    'Paper #': [f'Paper {i}' for i in df['Paper #']],\n",
        "    'Area Chair': df[\"Area Chair\"]  # Used for both color and marker symbol.\n",
        "})\n",
        "\n",
        "# Define a custom symbol sequence with enough unique symbols.\n",
        "symbol_seq = [\"circle\", \"square\", \"diamond\", \"cross\", \"x\", \"triangle-up\", \"triangle-down\",\n",
        "              \"triangle-left\", \"triangle-right\", \"pentagon\", \"hexagon\", \"star\"]\n",
        "\n",
        "fig = px.scatter(\n",
        "    df2,\n",
        "    x=\"Jittered Assessment\",\n",
        "    y=\"Meta Review Score\",\n",
        "    color=\"Area Chair\",           # Color by Area Chair.\n",
        "    symbol=\"Area Chair\",          # Marker symbol based on Area Chair.\n",
        "    symbol_sequence=symbol_seq,   # Provide a sequence of symbols.\n",
        "    hover_name=\"Paper #\",\n",
        "    hover_data=[\"Overall Assessment\", \"Area Chair\"],\n",
        "    title=\"Paired Distribution: Meta Review Score vs Overall Assessment\"\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQIdb6T8zAKo"
      },
      "source": [
        "### Correlation Between Scores (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DN9uQ7Gcy_mE"
      },
      "outputs": [],
      "source": [
        "def parse_avg(s):\n",
        "    try:\n",
        "        return float(s.split()[0])\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "# Build a temporary DataFrame with numeric values from the aggregated string columns.\n",
        "corr_data = pd.DataFrame({\n",
        "    \"Overall_Assessment_Avg\": df[\"Overall Assessment\"].apply(parse_avg),\n",
        "    \"Reviewer_Confidence_Avg\": df[\"Reviewer Confidence\"].apply(parse_avg),\n",
        "    \"Soundness_Score_Avg\": df[\"Soundness Score\"].apply(parse_avg),\n",
        "    \"Excitement_Score_Avg\": df[\"Excitement Score\"].apply(parse_avg),\n",
        "    # For Meta Review Score, try converting directly to float if possible.\n",
        "    \"Meta_Review_Score\": df[\"Meta Review Score\"].apply(lambda x: float(x) if isinstance(x, (int, float)) or (isinstance(x, str) and x.strip() != \"\") else np.nan)\n",
        "})\n",
        "\n",
        "# Compute the correlation matrix.\n",
        "corr_table = corr_data.corr()\n",
        "\n",
        "# Create a mask for the lower triangle of the correlation matrix.\n",
        "mask = np.tril(np.ones(corr_table.shape, dtype=bool))\n",
        "corr_table_upper = corr_table.mask(mask)\n",
        "\n",
        "print(\"Upper Triangle Correlation Table (lower triangle hidden):\")\n",
        "display(corr_table_upper)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nJC6fGKJJWz"
      },
      "source": [
        "## AC Dashboard\n",
        "\n",
        "*WARNING: run previous cell before running this one.*\n",
        "\n",
        "- Area Chair: the area chair\n",
        "- Completed Reviews: the number of completed reviews\n",
        "- Expected Reviews: the number of expected reviews (may more than 3 if emergency reviewers are assigned)\n",
        "- Papers Ready: the number of papers that have received three or more reviews\n",
        "- Num Papers: the total number of papers assigned to the area chair\n",
        "- All Reviews Ready: whether all papers have received three or more reviews\n",
        "- Meta Reviews Done: the number of completed meta-reviews\n",
        "- All Meta-reviews Ready: whether all meta-reviews are completed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ers6vL6Dr5LJ"
      },
      "outputs": [],
      "source": [
        "# Create numeric columns from \"Num Reviews\" by splitting the string.\n",
        "# This assumes every value is in the format \"x / y\"\n",
        "df[['CompRev', 'ExpRev']] = df[\"Num Reviews\"].str.split('/', expand=True)\n",
        "df['CompRev'] = df['CompRev'].astype(float)\n",
        "df['ExpRev'] = df['ExpRev'].astype(float)\n",
        "\n",
        "# --- Meta Table Aggregated by Area Chair ---\n",
        "meta_df = df.groupby(\"Area Chair\").agg(\n",
        "    Total_Completed_Reviews=(\"CompRev\", \"sum\"),\n",
        "    Total_Expected_Reviews=(\"ExpRev\", \"sum\"),\n",
        "    Papers_Ready=(\"Ready for Rebuttal\", lambda x: (x == \"‚úì\").sum()),\n",
        "    Num_Papers=(\"Paper #\", \"count\"),\n",
        "    Meta_Reviews_Num=(\"Meta Review Score\", lambda x: (x != \"\").sum())\n",
        ").reset_index()\n",
        "\n",
        "# Create a merged \"Num Reviews\" column in the format \"x / y\"\n",
        "meta_df[\"Num Reviews\"] = meta_df.apply(\n",
        "    lambda row: f\"{int(row['Total_Completed_Reviews'])} / {int(row['Total_Expected_Reviews'])}\",\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Add a column indicating if all papers are review-ready.\n",
        "meta_df[\"All Reviews Ready\"] = meta_df.apply(\n",
        "    lambda row: \"‚úì\" if row[\"Papers_Ready\"] == row[\"Num_Papers\"] else \"\", axis=1\n",
        ")\n",
        "\n",
        "# Format meta-review count as \"x of y\".\n",
        "meta_df[\"Meta_Reviews_Done\"] = meta_df.apply(\n",
        "    lambda row: f\"{row['Meta_Reviews_Num']} of {row['Num_Papers']}\", axis=1\n",
        ")\n",
        "\n",
        "# Add a column for meta-review readiness.\n",
        "meta_df[\"All Meta-reviews Ready\"] = meta_df.apply(\n",
        "    lambda row: \"‚úì\" if row[\"Meta_Reviews_Num\"] == row[\"Num_Papers\"] else \"\", axis=1\n",
        ")\n",
        "\n",
        "# Optionally drop the temporary numeric columns.\n",
        "meta_df.drop(columns=[\"Total_Completed_Reviews\", \"Total_Expected_Reviews\", \"Meta_Reviews_Num\"], inplace=True)\n",
        "\n",
        "# Reorder columns so that \"Num Reviews\" comes immediately after \"Area Chair\",\n",
        "# and \"Meta_Reviews_Done\" appears right before \"All Meta-reviews Ready\".\n",
        "desired_order = [\n",
        "    \"Area Chair\",\n",
        "    \"Num Reviews\",\n",
        "    \"Papers_Ready\",\n",
        "    \"Num_Papers\",\n",
        "    \"All Reviews Ready\",\n",
        "    \"Meta_Reviews_Done\",\n",
        "    \"All Meta-reviews Ready\"\n",
        "]\n",
        "meta_df = meta_df[desired_order]\n",
        "\n",
        "print(\"\\nMeta Table by Area Chair:\")\n",
        "display(meta_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8HfQbrVVlyE"
      },
      "source": [
        "## Check Confidential Comment & Review Issue Reports\n",
        "\n",
        "This might be helpful to keep an eye on those comments that requires special attention.\n",
        "\n",
        "**Update (Apr 3): Now I use forum-like style to present these comments in a user-friendly manner.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFeJHCDzVrU4"
      },
      "outputs": [],
      "source": [
        "def is_relevant_comment(reply):\n",
        "    invitations = reply.get(\"invitations\", [])\n",
        "    return any(\n",
        "        part in inv\n",
        "        for inv in invitations\n",
        "        for part in [\"/-/Author-Editor_Confidential_Comment\", \"/-/Comment\", \"/-/Review_Issue_Report\"]\n",
        "    )\n",
        "\n",
        "def classify_comment_type(reply):\n",
        "    invitations = reply.get(\"invitations\", [])\n",
        "    if any(\"/-/Review_Issue_Report\" in inv for inv in invitations):\n",
        "        return \"Review Issue\"\n",
        "    elif any(\"/-/Author-Editor_Confidential_Comment\" in inv for inv in invitations):\n",
        "        return \"Author-Editor Confidential\"\n",
        "    elif any(\"/-/Comment\" in inv for inv in invitations):\n",
        "        return \"Confidential Comment\"\n",
        "    else:\n",
        "        return \"Other\"\n",
        "\n",
        "def extract_comment_text(reply):\n",
        "    content = reply.get(\"content\", {})\n",
        "    # Try common keys in order of likely importance\n",
        "    for key in [\"comment\", \"justification\", \"text\", \"response\", \"value\"]:\n",
        "        if key in content:\n",
        "            val = content[key]\n",
        "            return val.get(\"value\") if isinstance(val, dict) else val\n",
        "    # If no known key, flatten all text fields into a fallback string\n",
        "    fallback = []\n",
        "    for k, v in content.items():\n",
        "        if isinstance(v, dict) and \"value\" in v:\n",
        "            fallback.append(f\"{k}: {v['value']}\")\n",
        "    return \"\\n\".join(fallback) if fallback else \"(No comment text found)\"\n",
        "\n",
        "\n",
        "def render_threaded_forum_grouped_by_paper(comments_df):\n",
        "    # Step 1: Build note_id ‚Üí row mapping\n",
        "    note_map = {row[\"NoteId\"]: row for _, row in comments_df.iterrows()}\n",
        "\n",
        "    # Step 2: Group comments by forum (paper thread)\n",
        "    paper_groups = comments_df.groupby(\"Paper #\")\n",
        "\n",
        "    html_blocks = []\n",
        "\n",
        "    for paper_number, group_df in paper_groups:\n",
        "        # Build children and root maps per paper\n",
        "        children_map = {}\n",
        "        roots = []\n",
        "        paper_note_map = {row[\"NoteId\"]: row for _, row in group_df.iterrows()}\n",
        "\n",
        "        for _, row in group_df.iterrows():\n",
        "            note_id = row[\"NoteId\"]\n",
        "            replyto = row[\"ReplyTo\"]\n",
        "            if replyto and replyto in paper_note_map:\n",
        "                children_map.setdefault(replyto, []).append(note_id)\n",
        "            else:\n",
        "                roots.append(note_id)\n",
        "\n",
        "        def render_comment(note_id, level=0):\n",
        "            row = paper_note_map[note_id]\n",
        "            indent = 20 * level\n",
        "            block = f\"\"\"\n",
        "            <div style=\"border:1px solid #ccc; border-radius:8px; padding:5px; margin:5px 0 5px {indent}px; width: 60%\">\n",
        "                <div style=\"font-weight:bold; color:#1a73e8;\">\n",
        "                    {row['Type']} <span style=\"color:gray; font-weight:normal;\">({row['Role']}, {row['Date']})</span>\n",
        "                </div>\n",
        "                <div style=\"margin:5px 0 10px;\">\n",
        "                    <a href=\"{row['Link']}\" target=\"_blank\">View on OpenReview ‚Üó</a>\n",
        "                </div>\n",
        "                <div style=\"white-space:pre-wrap; line-height:1.2;\">\n",
        "                    {md.markdown(row['Content'].strip())}\n",
        "                </div>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "            child_html = ''.join(render_comment(child_id, level + 1) for child_id in children_map.get(note_id, []))\n",
        "            return block + child_html\n",
        "\n",
        "        thread_html = ''.join(render_comment(root_id) for root_id in roots)\n",
        "\n",
        "        paper_header = f\"\"\"\n",
        "        <h3 style=\"border-bottom: 2px solid #999; padding-bottom: 4px;\">üìù Paper #{paper_number}</h3>\n",
        "        \"\"\"\n",
        "        html_blocks.append(paper_header + thread_html)\n",
        "\n",
        "    display(HTML(\"\".join(html_blocks)))\n",
        "\n",
        "def infer_role_from_signature(signatures):\n",
        "    if not signatures:\n",
        "        return \"Unknown\"\n",
        "    sig = signatures[0]  # Use first signature\n",
        "    if \"/Authors\" in sig:\n",
        "        return \"Author\"\n",
        "    elif \"/Reviewer\" in sig:\n",
        "        return \"Reviewer\"\n",
        "    elif \"/Area_Chair\" in sig:\n",
        "        return \"Area Chair\"\n",
        "    elif \"/Senior_Area_Chairs\" in sig:\n",
        "        return \"Senior Area Chair\"\n",
        "    elif \"/Program_Chairs\" in sig:\n",
        "        return \"Program Chair\"\n",
        "    elif sig.startswith(\"~\"):\n",
        "        return \"User\"\n",
        "    else:\n",
        "        return \"Other\"\n",
        "\n",
        "def format_timestamp(ms_since_epoch):\n",
        "    if not ms_since_epoch:\n",
        "        return \"\"\n",
        "    dt = datetime.fromtimestamp(ms_since_epoch / 1000)\n",
        "    return dt.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "\n",
        "# --- Collect relevant comments ---\n",
        "base_url = \"https://openreview.net/forum\"\n",
        "\n",
        "relevant_comments = []\n",
        "\n",
        "for submission in submissions:\n",
        "    submission_id = f\"{venue_id}/{submission_name}{submission.number}\"\n",
        "\n",
        "    for reply in submission.details.get(\"replies\", []):\n",
        "        if is_relevant_comment(reply):\n",
        "            forum_id = reply.get(\"forum\", \"\")\n",
        "            note_id = reply.get(\"id\", \"\")\n",
        "            replyto = reply.get(\"replyto\", None)\n",
        "            link = f\"{base_url}?id={forum_id}&noteId={note_id}\"\n",
        "            signatures = reply.get(\"signatures\", [])\n",
        "            role = infer_role_from_signature(signatures)\n",
        "            tcdate = reply.get(\"tcdate\", None)\n",
        "            date_str = format_timestamp(tcdate)\n",
        "\n",
        "            relevant_comments.append({\n",
        "                \"Paper #\": submission.number,\n",
        "                \"Paper ID\": submission.id,\n",
        "                \"Type\": classify_comment_type(reply),\n",
        "                \"Role\": role,\n",
        "                \"Date\": date_str,\n",
        "                \"Content\": extract_comment_text(reply),\n",
        "                \"Link\": f\"{base_url}?id={forum_id}&noteId={note_id}\",\n",
        "                \"ReplyTo\": reply.get(\"replyto\", None),\n",
        "                \"NoteId\": note_id,\n",
        "            })\n",
        "\n",
        "comments_df = pd.DataFrame(relevant_comments)\n",
        "render_threaded_forum_grouped_by_paper(comments_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNsWvPUgDsX9"
      },
      "source": [
        "### Comments in Table Form (optional)\n",
        "\n",
        "If you prefer to view these comments in an interactive table format, you can run the following code cell.\n",
        "Might be useful if you would like to filter some specific type of comments (for e.g., review issue report)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8WwCZakDwCV"
      },
      "outputs": [],
      "source": [
        "# Display as DataFrame (original one, optional)\n",
        "print(f\"Total relevant comments found: {len(comments_df)}\")\n",
        "display(comments_df.sort_values(by=\"Type\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
